# Computational workflows for the inference era
## Background

Formal computational workflows rarely prominently feature in scientific research outputs, even if they were part of the research methodology. 
Most benefits from adopting computational workflow software are obvious, such as clarity, automation and accessibility.
Many of the costs are also well known, such as inflexibility and efficiency. 
Despite these advantages, a number of recent studies in computational environmental sciences have developed their own project-specific workflow solutions  from scratch rather than adopting one of many existing workflow software (e.g. Kepler, Taverna, etc) that have been created specifically to meet these needs. 
These workflows share many common features, but are diverse in the specific features that are implemented. 
Here we analyse why similar scientific workflows have been reinvented so many times and show that this redundant effort highlights the absence of any single software with the essential features required to meet the demands of modern computational inference.
Building on our analysis, we identify features that the next generation of workflow software must possess in order to meet the demands of the inference age. 
These are 
(i) the separation of model structures, parameters and driver datasets 
(ii) support for multiple modes of abstraction within the same model structure 
(iii) the ability to overlay a traceability framework over model structures and monitor model behaviour on the basis of such a framework 
(iv) support for a variety of inference libraries, 
(v) the adoption of a provenance tracking rather than workflow paradigm 
(vi) the ability to structure computational workflows hierarchically 
(vii) distributed version control for models and data 
(viii) distributed and cloud based data access and retrieval 
(ix) remote computation and 
(x) a richer variety of data and model component interfaces.
